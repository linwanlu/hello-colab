{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqa/Jh8asyzsGz+/nWiXOr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linwanlu/hello-colab/blob/main/Hello_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 專題實作 #01：Hello Word"
      ],
      "metadata": {
        "id": "k6i_-aXF2Www"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Hello World!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnS29N683Iid",
        "outputId": "0d3f2687-b68d-42e0-f638-8f09fa43ea3e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##利用 Python 收集來自 API 的資料\n",
        "API 來源的爬蟲主要是通過網路上提供的 API 接口，從網路上獲取數據，例如從某個網站獲取股票市場行情、氣象預報等，通常是先向 API 接口發送請求，獲取 JSON 格式的數據，然後使用 json 模組對數據進行解析和處理。在今天的例子中，我們將以 Python 實現 API 來源的資料收集。若要進行 API 資料收集，可以使用 Python 中的 requests 套件向 API 伺服器發送 HTTP 請求，獲取 API 回傳的資料。以下是一個簡單的範例：\n"
      ],
      "metadata": {
        "id": "xPQb7SVl5Iik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "\n",
        "# 利用 requests 對 API 來源發送一個請求\n",
        "url = '{ API 網址}'\n",
        "response = requests.get(url)\n",
        "\n",
        "# 將請求回應的內容存成一個字串格式\n",
        "d = response.text\n",
        "\n",
        "# 將長得像 json 格式的字串解析成字典或列表\n",
        "data = json.loads(d)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "4Srh_8mj5Tm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上範例中的 `data` 變數，就是我們利用 Python 抓取回來的資料並且經過解析後變成一個 Python 內建的資料結構。但是有這樣的資料之後，下一步還需要做什麼呢？這個時候就會需要仰賴你對程式語言的熟悉程度，將原始的資料整理成你想要使用的樣子。  \n",
        "\n"
      ],
      "metadata": {
        "id": "QElRtk7f5i-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "除此之外，你還有利用更多的第三方套件與函式庫協助你在資料收集的項目中完成更高階的操作：\n",
        "\n",
        "\n",
        "* requests：用於向網站發送 HTTP 請求，獲取網站上的資訊。\n",
        "* BeautifulSoup：用於解析 HTML 或 XML 結構化資料，抓取所需的資料。\n",
        "* Scrapy：用於編寫網頁爬蟲框架，支持非同步網路爬蟲。\n",
        "* Selenium：用於自動化瀏覽器操作，支持 JavaScript 用於模擬操作網頁的行為解決網頁動態加載的問題。\n",
        "* pandas：用於數據處理和分析，可將爬蟲收集到的數據進行結構化處理和分析。\n",
        "\n",
        "\n",
        "\n",
        " 總體來說，使用 Python 進行資料爬蟲和資料收集是一個相對簡單而且實用的技能，可以幫助我們獲取網路上的資訊，並對這些資訊進行處理、分析和可視化，從而加速日常工作的生產力。"
      ],
      "metadata": {
        "id": "Mycsjw-z5rK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task #07 作業#01 口罩資料API串接"
      ],
      "metadata": {
        "id": "KuqIT4Az7dcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url='https://raw.githubusercontent.com/kiang/pharmacies/master/json/points.json'\n",
        "response = requests.get(url)\n",
        "\n",
        "d=response.text\n",
        "\n",
        "data = json.loads(d)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwQyjoQ57qJG",
        "outputId": "5383922b-2135-436b-dff6-298ba21e98e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}